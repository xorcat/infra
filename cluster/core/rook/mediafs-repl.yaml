#################################################################################################################
# Create a filesystem with settings for erasure-coding instead of replication. A minimum of three nodes with OSDs
# are required in this example since the default failureDomain is host.
#  kubectl create -f filesystem-ec.yaml
#################################################################################################################
---
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: mediafs
  namespace: rook-ceph # namespace:cluster
spec:
  # The metadata pool spec
  metadataPool:
    deviceClass: ssd
    replicated:
      # You need at least three OSDs on different nodes for this config to work
      size: 3
  # The list of data pool specs
  dataPools:
    - name: repl
      # deviceClass: hdd
      replicated:
        size: 2
        hybridStorage:
          primaryDeviceClass: ssd
          secondaryDeviceClass: hdd
      # Inline compression mode for the data pool
      parameters:
        compression_mode: none
      # deviceClass: hdd
  # Whether to preserve filesystem after CephFilesystem CRD deletion
  preserveFilesystemOnDelete: true
  # The metadata service (mds) configuration
  metadataServer:
    # The number of active MDS instances
    activeCount: 2
    # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
    # If false, standbys will be available, but will not have a warm cache.
    activeStandby: true
    # The affinity rules to apply to the mds deployment
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: role
                  operator: NotIn
                  values:
                    - slow-node
      #    requiredDuringSchedulingIgnoredDuringExecution:
      #      nodeSelectorTerms:
      #      - matchExpressions:
      #        - key: role
      #          operator: In
      #          values:
      #          - mds-node
      #  topologySpreadConstraints:
      #  tolerations:
      #  - key: mds-node
      #    operator: Exists
      #  podAffinity:
      podAntiAffinity:
        # requiredDuringSchedulingIgnoredDuringExecution:
        #   - labelSelector:
        #       matchExpressions:
        #         - key: app
        #           operator: In
        #           values:
        #             - rook-ceph-mds
        #     # topologyKey: kubernetes.io/hostname will place MDS across different hosts
        #     topologyKey: kubernetes.io/hostname
        # preferredDuringSchedulingIgnoredDuringExecution:
        #   - weight: 100
        #     podAffinityTerm:
        #       labelSelector:
        #         matchExpressions:
        #           - key: app
        #             operator: In
        #             values:
        #               - rook-ceph-mds
        #       # topologyKey: */zone can be used to spread MDS across different AZ
        #       # Use <topologyKey: failure-domain.beta.kubernetes.io/zone> in k8s cluster if your cluster is v1.16 or lower
        #       # Use <topologyKey: topology.kubernetes.io/zone>  in k8s cluster is v1.17 or upper
        #       topologyKey: kubernetes.io/hostname
    # A key/value list of annotations
    annotations:
    #  key: value
    # A key/value list of labels
    labels:
    #  key: value
    resources:
      # The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory
      limits:
        cpu: "2"
        memory: "4Gi"
      requests:
        cpu: 500m
        memory: "1Gi"
    # priorityClassName: my-priority-class
